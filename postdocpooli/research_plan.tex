% Use document class "aletter.cls" (for letters):
\documentclass[11pt, oneside]{essay}

% ISO 8859-1 character encoding is assumed
\usepackage[finnish, english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage{color}
\usepackage{amsmath, amssymb}

% shortcuts for math
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\phi}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\qlay}[1]{\left[#1\right]}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vt}[0]{\vect{t}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mC}{\matr{C}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}
\newcommand{\td}[0]{\text{d}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vzero}[0]{\vect{0}}
%\newcommand{\tf}[0]{\text{f}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\TT}[0]{{\vects{\theta}}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}
\newcommand{\tred}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Deep Learing for Statistical Machine Translation}
\author{KyungHyun Cho}

\begin{document}

\maketitle

\section{Introduction}





\section{Background}

\subsection{Deep Learning}

\subsection{Statistical Machine Translation}


\section{Neural Networks: Language Modeling to Statistical
Machine Translation}

In this section, I briefly describe how neural networks may be
applied to the task of statistical machine translation (SMT) by
first starting with a probabilistic framework behind SMT. Then, I
introduce some earlier works that have applied neural networks to
the task of language modeling which is an important component of
the SMT system.

At the end of this section, I propose and discuss a novel
approach that attempts to replace the whole existing SMT
framework with a new framework based purely on neural networks.
This approach is the basis on which research during my
post-doctoral years at the University of Montreal will be
conducted.

\subsection{Probabilistic Framework behind Statistical Machine
Translation}

A statistical machine translation (SMT) system may be described
in a single equation:
\begin{align*}
    \vt^* = \argmax_{\vt} p(\vt \mid \vs),
\end{align*}
where $\vs$ and $\vt$ are respectively the source and target
sentences, and $\vt^*$ is the most likely target sentence which
corresponds to the correct translated sentence under the SMT
system. It is common to decompose this equation into an
inverse-translation probability and a language modeling using
Bayes' rule such that
\begin{align}
    \label{eq:smt}
    \vt^* =& \argmax_{\vt} p(\vt \mid \vs) 
    \nonumber
    \\
    =& \argmax_{\vt} p(\vs \mid \vt) p(\vt),
\end{align}
where $p(\vs \mid \vt)$ is the inverse-translation probability
and $p(\vt)$ is the language model of the target language
\citep{Koehn2010}.

In this probabilistic framework, the task of SMT is broken down
into two parts; (1) inference and (2) learning. 

Inference corresponds to the actual task of \textit{translating} a
given sentence $\vs$ into a foreign sentence $\vt^*$. The best
target sentence $\vt^*$ can be found by search for a pair
$\left(\vs, \vt\right)$ that gives the highest
\textit{probability} $p(\vt \mid \vs)$ given by the model in
Eq.~\eqref{eq:smt}.

Learning is used to actually \textit{learn} $p(\vt \mid \vs)$
from data.  Often, one uses separate models for the
inverse-translation model $p(\vs \mid \vt)$ and the language
model $p(\vt)$ and combines them as a log-linear model:
\begin{align}
    \label{eq:loglinear}
    p(\vt \mid \vs) \propto \exp\left\{ \omega_0 \log p(\vs \mid
    \vt) + \omega_1 \log p(\vt) \right\}, 
\end{align}
where $\omega_0$ and $\omega_1$ are the weights for the
inverse-translation model and the language model, respectively.
In this case, learning may be decomposed into learning the
inverse-translation model, learning the language model and
learning their weights.

The inverse-translation model is conventionally implemented by a
phrase-based approach \citep[see, e.g.,][]{Koehn2003,Marcu2002}.
In the phrase-based approach, the inverse-translation probability
is decomposed into the probabilities of short phrases:
\begin{align*}
    p(\vs \mid \vt) = \prod_{i=1}^{I} \phi(s_i \mid t_i) d(a_i -
    b_i -1),
\end{align*}
where $phi(s_i \mid t_i)$ and $d(a_i - b_i - 1)$ are the phrase
translation probability and the reordering weight based on the
start and the end of the $i$-th phrase of the source sentence
$s_i$. Here, it is assumed that the sentence is decomposed into
$I$ phrases. Learning this model, thus, consists of (1) learning
the phrase translation model and (2) deciding/learning the
reordering model.

There are rather diverse approaches to learning the language
model. The most naive approach, called $n$-gram language model,
assumes a $n$-th order Markov property and decomposes the
language model $p(\vt)$ into
\begin{align}
    \label{eq:ngram}
    p(\vt) = \prod_{l=1}^L p(t_l \mid t_{l-n}, \dots, t_{l-1}),
\end{align}
where each $t_l$ corresponds to the $l$-th word in a sentence
$\vt$. Then, each $n$-gram probability $p(t_l \mid t_{l-n},
\dots, t_{l-1})$ can be learned from data by simply counting the
number of occurences of the $n$-gram. 

This $n$-gram approach is a \textit{discrete}-space approach in
the sense that the $n$-gram model can be effectively represented
by a table with a finite number of entries. Unfortunately, this
approach is limited by the fact that as $n$ increase, the number
of unseen $n$-grams increases, which affects the overall
performance of SMT system relying on this language model badly. 

Alternatively, one may resort to a continuous-space language
model. In this approach, one models each term in
Eq.~\eqref{eq:ngram} with, for instance, a neural network
\citep{Bengio2003}. The neural network trained with data will
then be able to interpolate and extrapolate to generalize to
unseen $n$-grams. Furthermore, one can use a neural network that
does not require the assumption of Markov property, such as
recurrent neural networks (RNN) \citep{Rumelhart1986}. 

In the next subsection, I describe a number of approaches to
language modeling based on neural networks. This approach based
on neural networks is important, as it will constitute a basis on
which my research goal will be conducted. 

\subsection{Neural Networks for Language Modeling}

\subsubsection{Feedforward Neural Network}

A neural network can model the $n$-gram model in
Eq.~\eqref{eq:ngram}, by learning a nonlinear mapping from $n$
previous words $\left\{ t_{l-n}, \dots, t_{l-1} \right\}$ to the
$l$-th word $t_l$. \citet{Bengio2003} proposed a feedforward
neural network architecture that subsequently performs (1)
projecting the previous $n$ words into a common continuous-space
representation and (2) combining them to predict the probability
of the next word $t_l$. 

Mathematically, the $n$-gram model using a feedforward neural
network can be written as, for instance, 
\begin{align*}
    p(t_l =t \mid t_{l-n}, \dots, t_{l-1}) = \text{softmax}
    \left( \mW^\top \tanh\left(\sum_{i=1}^n \mC^\top t_{l-i} +
    \vb\right) +\vc
    \right),
\end{align*}
where $\mC$ is the word embedding matrix shared by all $n$
previous words. $\vb$ and $\vc$ are bias vectors, and $\mW$ is
the weight parameter.

\tred{TODO}

\subsubsection{Recurrent Neural Networks}

A recurrent neural network (RNN) is able to simulate a discrete
dynamical system that has an input $\vx_t$, an output $\vy_t$ and
a hidden state $\vh_t$.  The dynamical system is defined by
\begin{align}
    \label{eq:dynamical_system_trans}
    \vh_t &= f_h(\vx_t, \vh_{t-1}) 
    \\
    \label{eq:dynamical_system_out}
    \vy_t &= f_o(\vh_t),
\end{align}
where $f_h$ and $f_o$ are a state transition function and an
output function, respectively. Each function is parameterized by
a set of parameters; $\TT_h$ and $\TT_o$, respectively. 

If an RNN is trained to output, or predict, the input at the next
timestep $t+1$, one may directly use the RNN to learn the
language model $p(\vt)$ in Eq.~\eqref{eq:smt} without the Markov
assumption. This is possible, since the output $\vy_t$ depends on
all previous inputs $\left\{ \vx_1, \dots, \vx_{t-1} \right\}$.

When the language model $p(\vt)$ is decomposed into
\[
    p(\vt) = \prod_{l=1}^L p(t_l \mid t_{l-1}, \dots, t_1),
\]
an RNN can learn this language model by 
\[
    p(t_l \mid t_{l-1}, \dots, t_1) = f_o(f_h(t_{l-1},
    f_h(t_{l-2}, \dots, f_h(t_1, \vzero)))),
\]
which is an unfolded expression of the RNN given in
Eqs.~\eqref{eq:dynamical_system_trans}--\eqref{eq:dynamical_system_out}
\citep{Mikolov2010}.

\tred{TODO}


\subsection{Neural Networks for Statistical Machine Translation}

The language models based on neural networks, both feedforward
and recurrent, have been found to improve the overal translation
performance \tred{\citep{??,??,??}}. However, in the overall
framework of statistical machine translation (SMT), the language
model $p(\vt)$ constitutes only a small part (See
Eq.~\eqref{eq:smt}). Effectively, the probability computed by the
language model is just a single term in the score of a candiate,
target sentence (See Eq.~\eqref{eq:loglinear}). 

The widely used phrase-based translation system, for instance,
rather puts more emphasis on building a phrase translation table,
estimating phrase translation probabilities



\section{Research Plan}

\subsection{Goals}

\subsection{Hosting Institute: Machine Learning Lab at University of Montreal}

\subsection{Timeline}







\small
\bibliographystyle{research_plan}
\bibliography{research_plan}





\end{document}
