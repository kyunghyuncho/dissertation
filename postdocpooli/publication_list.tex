% Use document class "aletter.cls" (for letters):
\documentclass[11pt, oneside]{essay}

% ISO 8859-1 character encoding is assumed
\usepackage[finnish, english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage{color}
\usepackage{amsmath, amssymb}

\newcommand{\tred}[1]{\textcolor{red}{#1}}

\title{Publication list}
\author{KyungHyun Cho}

\begin{document}

\maketitle

\vspace{5mm}

- \textbf{KyungHyun Cho}, Tapani Raiko and Alexander Ilin.
\textbf{Enhanced Gradient and Adaptive Learning Rate for Training
Restricted Boltzmann Machines}.
in the proceedings of the International Conference on
Machine Learning (ICML 2011), Bellevue, Washington, USA,
28 June-02 July, 2011. (to appear) 

- Tapani Raiko, \textbf{KyungHyun Cho} and Alexander Ilin.
\textbf{Enhanced Gradient for Learning Boltzmann Machines}.
The Learning Workshop 2011, Fort Lauderdale, Florida, USA, 13-16
April, 2011. (Abstract only)

\begin{center}
\begin{minipage}{0.93\textwidth}

\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}

\textbf{Description}:
In this paper, I with the co-authors proposed a
new training algorithm consisting of the enhanced gradient
and the adaptive learning. The new training algorithm was
shown to overcome some crucial difficulties in training
restricted Boltzmann machines (RBM) which include the
difficult choice of learning hyper-parameters, the
easily diverging behavior, and the difficulty of avoiding
bad local solutions. 

The adaptive learning rate which automatically adjusts
step-size of stochastic gradient updates on-the-fly based on
local estimates of log-likelihood was designed and proposed
by me and was shown to have a stable behavior regardless of
the choice of the initial learning rate. Also, I helped in
deriving a new update rule, the enhanced gradient, for RBMs,
which avoids bad local solutions by exploiting the
geometry of the parameter space of RBMs.

\end{minipage}
\end{center}

\vspace{5mm}

- \textbf{KyungHyun Cho}, Alexander Ilin and Tapani Raiko.
\textbf{Improved Learning of Gaussian-Bernoulli Restricted Boltzmann
Machines}.
in the proceedings of the International Conference on
Artificial Neural Networks (ICANN 2011), Espoo, Finland,
14-17 June, 2011. (to appear)


\begin{center}
\begin{minipage}{0.93\textwidth}

\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}

\textbf{Description}:
Gaussian-Bernoulli RBMs (GBRBM) which can learn continuous
data unlike the conventional RBM have been known to be
difficult to train. In this work, the authors proposed a new
parameterization and algorithm of the model and its learning
as well as explanations on why it has not been easy to train
GBRBM.

I mainly focused on explaining the difficulties and their
underlying causes, and designed the experiments revealing
that the proposed learning algorithms are indeed able to
address those problems. Also, GBRBM was implemented to
utilize computational power from the workstations with
graphical processing units (GPU) in order to speed up the
experiments.

\end{minipage}
\end{center}

\vspace{5mm}
\newpage

- \textbf{KyungHyun Cho}, Tapani Raiko and Alexander Ilin.
\textbf{Parallel Tempering is Efficient for Learning
Restricted Boltzmann Machines}.
in the proceedings of the International Joint Conference on
Neural Networks (IJCNN 2010), Barcelona, Spain, 18-23 July,
2010.


\begin{center}
\begin{minipage}{0.93\textwidth}

\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}

\textbf{Description}:
Contrastive divergence (CD) learning proposed by Prof.
Hinton in 2002 has been the choice of learning algorithm for
training RBMs for long, however, with a limitation that it
is biased so that the trained RBM does not learn a good
generative model. Hence, the co-authors and I have come up
with an idea to replace Gibbs sampler in CD learning with an
advanced sampler called parallel tempering (PT). 

I implemented a new PT learning algorithm for training
restricted Boltzmann machines and conducted extensive
experiments comparing PT learning against CD learning. Also,
a simple, but straight-forward, experimental measure for
determining whether a good generative model is learned was
developed.

\end{minipage}
\end{center}



\end{document}




