Deep neural networks have become increasingly popular
under the name of deep learning recently due to their
success in challenging machine learning tasks.  Although the
popularity is mainly due to recent successes, the
history of neural networks goes as far back as 1958 when
Rosenblatt presented a perceptron learning algorithm.  Since
then, various kinds of artificial neural networks have been
proposed. They include Hopfield networks, self-organizing
maps, neural principal component analysis, Boltzmann
machines, multi-layer perceptrons, radial-basis function
networks, autoencoders, sigmoid belief networks, support
vector machines and deep belief networks.

In the first part of this thesis, the author aims at
investigating these models and finding a common set of basic
principles for deep neural networks. The thesis starts from
some of the earlier ideas and models in the field of
artificial neural networks and arrive at autoencoders and
Boltzmann machines which are two most widely studied neural
networks these days. The author thoroughly discusses how
those various neural networks are related to each other and
how the principles behind those networks form a foundation for
autoencoders and Boltzmann machines. 

The second part is the collection of the ten recent
publications by the author. These publications mainly focus
on learning and inference algorithms of Boltzmann machines
and autoencoders. Especially, Boltzmann machines, which are
known to be difficult to train, have been in the main focus.
Throughout several publications the author and the
co-authors have devised and proposed a new set of learning
algorithms which includes the enhanced gradient, adaptive
learning rate and parallel tempering. These algorithms are
further applied to a restricted Boltzmann machine with
Gaussian visible units.
%which is deemed to be more difficult to train
%compared to a binary one. 

In addition to these algorithms for restricted Boltzmann
machines the author proposed a two-stage pretraining
algorithm that initializes the parameters of a deep
Boltzmann machine to match the variational posterior
distribution of a similarly structured deep autoencoder.
Finally, deep neural networks are applied to image denoising
and speech recognition.






